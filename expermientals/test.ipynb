{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 182\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHandling: \u001b[39m\u001b[39m{\u001b[39;00mitem\u001b[39m}\u001b[39;00m\u001b[39m now!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    175\u001b[0m now \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    176\u001b[0m summaries \u001b[39m=\u001b[39m {\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mimg_1\u001b[39m\u001b[39m\"\u001b[39m: item[\u001b[39m\"\u001b[39m\u001b[39mimg_1\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mimg_2\u001b[39m\u001b[39m\"\u001b[39m: item[\u001b[39m\"\u001b[39m\u001b[39mimg_2\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m    181\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: item[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m--> 182\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mimg_1_summa\u001b[39m\u001b[39m\"\u001b[39m: get_lumance_summary_2d(item[\u001b[39m\"\u001b[39;49m\u001b[39mimg_1\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\n\u001b[0;32m    183\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mimg_2_summa\u001b[39m\u001b[39m\"\u001b[39m : get_lumance_summary_2d(item[\u001b[39m\"\u001b[39m\u001b[39mimg_2\u001b[39m\u001b[39m\"\u001b[39m]),\n\u001b[0;32m    184\u001b[0m \u001b[39m\"\u001b[39m\u001b[39mmodel_summa\u001b[39m\u001b[39m\"\u001b[39m : get_lumance_summary_3d(item[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    185\u001b[0m }\n\u001b[0;32m    186\u001b[0m summaries_rows\u001b[39m.\u001b[39mappend(summaries)\n\u001b[0;32m    187\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIt takes \u001b[39m\u001b[39m{\u001b[39;00mtime\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m now\u001b[39m}\u001b[39;00m\u001b[39m seconds for this row\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn [1], line 25\u001b[0m, in \u001b[0;36mget_lumance_summary_2d\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_lumance_summary_2d\u001b[39m(img_path):\n\u001b[0;32m     24\u001b[0m     pic_f_image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(img_path)\u001b[39m.\u001b[39mresize((\u001b[39m450\u001b[39m, \u001b[39m300\u001b[39m), Image\u001b[39m.\u001b[39mLANCZOS)\n\u001b[1;32m---> 25\u001b[0m     ceremicsmask_predictor \u001b[39m=\u001b[39m MaskPredictor(\u001b[39m\"\u001b[39;49m\u001b[39m../computation/ceremicsmask.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m     \u001b[39m#Getting the numpy arrys for the image and the mask\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     img_mask_np \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(ceremicsmask_predictor\u001b[39m.\u001b[39mpredict(pic_f_image))\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\development\\APSAP_Sherd_Matching_Tool\\expermientals\\..\\computation\\nn_segmentation.py:34\u001b[0m, in \u001b[0;36mMaskPredictor.__init__\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m./model/ceremicsmask.pt\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     33\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m     loaded_model \u001b[39m=\u001b[39m get_model_instance_segmentation(\u001b[39m2\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m     36\u001b[0m         loaded_model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_path, map_location\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)))\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\development\\APSAP_Sherd_Matching_Tool\\expermientals\\..\\computation\\nn_segmentation.py:20\u001b[0m, in \u001b[0;36mget_model_instance_segmentation\u001b[1;34m(num_classes)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_model_instance_segmentation\u001b[39m(num_classes):\n\u001b[1;32m---> 20\u001b[0m     model \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mdetection\u001b[39m.\u001b[39;49mmaskrcnn_resnet50_fpn(weights\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDEFAULT\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     21\u001b[0m     in_features \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mroi_heads\u001b[39m.\u001b[39mbox_predictor\u001b[39m.\u001b[39mcls_score\u001b[39m.\u001b[39min_features\n\u001b[0;32m     22\u001b[0m     model\u001b[39m.\u001b[39mroi_heads\u001b[39m.\u001b[39mbox_predictor \u001b[39m=\u001b[39m FastRCNNPredictor(in_features, num_classes)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torchvision\\models\\_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    136\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing \u001b[39m\u001b[39m{\u001b[39;00msequence_to_str(\u001b[39mtuple\u001b[39m(keyword_only_kwargs\u001b[39m.\u001b[39mkeys()), separate_last\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mand \u001b[39m\u001b[39m'\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m as positional \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    138\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstead.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    139\u001b[0m     )\n\u001b[0;32m    140\u001b[0m     kwargs\u001b[39m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[1;32m--> 142\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torchvision\\models\\_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[39mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[0;32m    226\u001b[0m     kwargs[weights_param] \u001b[39m=\u001b[39m default_weights_arg\n\u001b[1;32m--> 228\u001b[0m \u001b[39mreturn\u001b[39;00m builder(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torchvision\\models\\detection\\mask_rcnn.py:500\u001b[0m, in \u001b[0;36mmaskrcnn_resnet50_fpn\u001b[1;34m(weights, progress, num_classes, weights_backbone, trainable_backbone_layers, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m model \u001b[39m=\u001b[39m MaskRCNN(backbone, num_classes\u001b[39m=\u001b[39mnum_classes, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    499\u001b[0m \u001b[39mif\u001b[39;00m weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 500\u001b[0m     model\u001b[39m.\u001b[39mload_state_dict(weights\u001b[39m.\u001b[39;49mget_state_dict(progress\u001b[39m=\u001b[39;49mprogress))\n\u001b[0;32m    501\u001b[0m     \u001b[39mif\u001b[39;00m weights \u001b[39m==\u001b[39m MaskRCNN_ResNet50_FPN_Weights\u001b[39m.\u001b[39mCOCO_V1:\n\u001b[0;32m    502\u001b[0m         overwrite_eps(model, \u001b[39m0.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torchvision\\models\\_api.py:66\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[1;34m(self, progress)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state_dict\u001b[39m(\u001b[39mself\u001b[39m, progress: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Mapping[\u001b[39mstr\u001b[39m, Any]:\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m load_state_dict_from_url(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murl, progress\u001b[39m=\u001b[39;49mprogress)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torch\\hub.py:735\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[1;34m(url, model_dir, map_location, progress, check_hash, file_name)\u001b[0m\n\u001b[0;32m    733\u001b[0m \u001b[39mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[0;32m    734\u001b[0m     \u001b[39mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location)\n\u001b[1;32m--> 735\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(cached_file, map_location\u001b[39m=\u001b[39;49mmap_location)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torch\\serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
      "File \u001b[1;32mc:\\Users\\bertliu\\.conda\\envs\\apsap\\lib\\site-packages\\torch\\serialization.py:1020\u001b[0m, in \u001b[0;36m_legacy_load\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1018\u001b[0m \u001b[39massert\u001b[39;00m key \u001b[39min\u001b[39;00m deserialized_objects\n\u001b[0;32m   1019\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[key]\n\u001b[1;32m-> 1020\u001b[0m typed_storage\u001b[39m.\u001b[39;49m_storage\u001b[39m.\u001b[39;49m_set_from_file(\n\u001b[0;32m   1021\u001b[0m     f, offset, f_should_read_directly,\n\u001b[0;32m   1022\u001b[0m     torch\u001b[39m.\u001b[39;49m_utils\u001b[39m.\u001b[39;49m_element_size(typed_storage\u001b[39m.\u001b[39;49mdtype))\n\u001b[0;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m offset \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m     offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import ctypes\n",
    "\n",
    "opengl_path = \"../computation/opengl32.dll\"\n",
    "ctypes.cdll.LoadLibrary(opengl_path)\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import imagehash\n",
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "from computation.nn_segmentation import MaskPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "from debug_database import get_all_pottery_sherd_info\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "print(len(get_all_pottery_sherd_info()))\n",
    "def get_lumance_summary_2d(img_path):\n",
    "    pic_f_image = Image.open(img_path).resize((450, 300), Image.LANCZOS)\n",
    "    ceremicsmask_predictor = MaskPredictor(\"../computation/ceremicsmask.pt\")\n",
    "    #Getting the numpy arrys for the image and the mask\n",
    "    img_mask_np = np.array(ceremicsmask_predictor.predict(pic_f_image))\n",
    "    img_np = np.array(pic_f_image.convert('RGBA'))\n",
    "    #Cleaning the background\n",
    "    for i in range(len(img_mask_np)):\n",
    "        for j in range(len(img_mask_np[0])):\n",
    "           if img_mask_np[i][j]  <= 170:\n",
    "                img_np[i][j][0] = 0\n",
    "                img_np[i][j][1] = 0\n",
    "                img_np[i][j][2] = 0\n",
    "                img_np[i][j][3] = 0\n",
    "    image =   Image.fromarray((img_np))\n",
    "\n",
    "    np_image =np.array(image.convert('L')).ravel()\n",
    "    \n",
    "    #gather lumance summary\n",
    "    lumance_summary = []\n",
    " \n",
    "    for i in range(0, 256):\n",
    "        lumance_summary.append(0)\n",
    "    for luminance in np_image:\n",
    "        if luminance != 0:\n",
    "            lumance_summary[luminance] += 1\n",
    " \n",
    " \n",
    "    return lumance_summary\n",
    "\n",
    "\n",
    "def get_lumance_summary_3d(model_path, ply_window):\n",
    "\n",
    "    #load the model\n",
    "    current_pcd_load = o3d.io.read_point_cloud(model_path) \n",
    "    ply_window.add_geometry(current_pcd_load)\n",
    "    ctr = ply_window.get_view_control()    \n",
    "    ply_window.get_render_option().point_size = 5\n",
    "    ctr.change_field_of_view(step=-9)\n",
    "\n",
    "    img_captured = ply_window.capture_screen_float_buffer(True)\n",
    "    image_np =  np.multiply(np.array(img_captured), 255).astype(np.uint8)\n",
    "    img = Image.fromarray(image_np)\n",
    "    \n",
    "    img_np = np.array(img.convert('RGBA'))\n",
    "    \n",
    "    for i in range(len(img_np)):\n",
    "        for j in range(len(img_np[0])):\n",
    "           if img_np[i][j][0]  == 255 and img_np[i][j][1]  == 255 and img_np[i][j][2]  == 255:\n",
    "                img_np[i][j][0] = 0\n",
    "                img_np[i][j][1] = 0\n",
    "                img_np[i][j][2] = 0\n",
    "                img_np[i][j][3] = 0\n",
    "    image =   Image.fromarray((img_np))\n",
    " \n",
    "    np_image = np.array(image.convert('L')).ravel()\n",
    "    \n",
    "\n",
    "    #clean up the model\n",
    "\n",
    "    ply_window.remove_geometry(current_pcd_load)\n",
    "    del ctr\n",
    "\n",
    "    #generate the summary and return the result\n",
    " \n",
    "    \n",
    "    #gather lumance summary\n",
    "    lumance_summary = []\n",
    "    for i in range(0, 256):\n",
    "        lumance_summary.append(0)\n",
    "    for luminance in np_image:\n",
    "        if luminance != 0:\n",
    "            lumance_summary[luminance] += 1\n",
    "    return lumance_summary\n",
    "import json\n",
    "\"\"\"\n",
    "all_images_urls = []\n",
    "ceremicsmask_predictor = MaskPredictor(\"../computation/ceremicsmask.pt\")\n",
    "\"\"\"\n",
    "model_path = r\"D:\\ararat\\data\\files\\N\\38\\478020\\4419550\\11\\finds\\3dbatch\\2022\\batch_000\\registration_reso1_maskthres242\\final_output\\piece_1_world.ply\"\n",
    "\n",
    "img_path = r\"D:\\ararat\\data\\files\\N\\38\\478020\\4419550\\11\\finds\\individual\\1\\photos\\1.jpg\"\n",
    "\"\"\"\n",
    "_3d_summa = get_lumance_summary_3d(model_path)\n",
    "_2d_summa = get_lumance_summary_2d(img_path)\n",
    " \n",
    "normalized_3d_summa = (np.array(_3d_summa)/sum(_3d_summa))\n",
    "\n",
    "normalized_2d_summa = (np.array(_2d_summa)/sum(_2d_summa))\n",
    "\n",
    "#Now since all numbers are like to be fraction, like 0.02, 0.314, it would be good if we can normalized them between 0 and 1 \n",
    "min_3d = min(normalized_3d_summa)\n",
    "max_3d = max(normalized_3d_summa)\n",
    "\n",
    "min_2d = min(normalized_2d_summa)\n",
    "max_2d = max(normalized_2d_summa)\n",
    "\n",
    "print((normalized_3d_summa - min_3d)/(max_3d - min_3d))\n",
    "print((normalized_2d_summa - min_2d)/(max_2d - min_2d))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "my_file = \"./allShardsData.json\"\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open(my_file)\n",
    "ply_window = o3d.visualization.Visualizer()\n",
    "\n",
    "ply_window.create_window(visible=False, width=430, height=390)   \n",
    "ply_window.get_render_option().light_on = False\n",
    " \n",
    "data = json.load(f)\n",
    " \n",
    "kc = {'4780204419550', '4781304419430', '4782304419460'}\n",
    "rows = []\n",
    "\n",
    "packed_2d_3d = []\n",
    "\n",
    "for row in data:\n",
    "    if(row[-3] != None and row[-2]!= None):\n",
    "        hemisphere = row[0]\n",
    "        zone = row[1]\n",
    "        easting = row[2]\n",
    "        northing = row[3]\n",
    "        trench = row[4]\n",
    "        piece_2d = row[5]\n",
    "        batch = row[-3]\n",
    "        piece_3d = row[-2]\n",
    "        img_1 =  rf\"D:\\ararat\\data\\files\\{hemisphere}\\{zone}\\{easting}\\{northing}\\{trench}\\finds\\individual\\{piece_2d}\\photos\\1.jpg\"\n",
    "       \n",
    "        img_2 =  rf\"D:\\ararat\\data\\files\\{hemisphere}\\{zone}\\{easting}\\{northing}\\{trench}\\finds\\individual\\{piece_2d}\\photos\\2.jpg\"\n",
    "         \n",
    "        model_path = rf\"D:\\ararat\\data\\files\\{hemisphere}\\{zone}\\{easting}\\{northing}\\{trench}\\finds\\3dbatch\\2022\\batch_{int(batch):03}\\registration_reso1_maskthres242\\final_output\\piece_{piece_3d}_world.ply\"\n",
    "        if Path(img_1).is_file() and Path(img_2).is_file() and Path(model_path).is_file():\n",
    "            packed ={\n",
    "                \"img_1\": img_1,\n",
    "                \"img_2\": img_2,\n",
    "                \"model\" : model_path\n",
    "            }\n",
    "            packed_2d_3d.append(packed)\n",
    "\n",
    "import json\n",
    "with open('matched.json', 'w') as f:\n",
    "    json.dump(packed_2d_3d, f)\n",
    "import time\n",
    "\n",
    "summaries_rows = []\n",
    "for item in packed_2d_3d:\n",
    "    print()\n",
    "    print(f\"Handling: {item} now!\")\n",
    "    now = time.time()\n",
    "    summaries = {\n",
    "\n",
    "    \"img_1\": item[\"img_1\"],\n",
    "\n",
    "    \"img_2\": item[\"img_2\"],\n",
    "    \"model\": item[\"model\"],\n",
    "    \"img_1_summa\": get_lumance_summary_2d(item[\"img_1\"]),\n",
    "    \"img_2_summa\" : get_lumance_summary_2d(item[\"img_2\"]),\n",
    "    \"model_summa\" : get_lumance_summary_3d(item[\"model\"], ply_window)\n",
    "    }\n",
    "    summaries_rows.append(summaries)\n",
    "    print(f\"It takes {time.time() - now} seconds for this row\")\n",
    "    print(summaries)\n",
    "    print()\n",
    "with open('summaries.json', 'w') as f:\n",
    "    json.dump(summaries_rows, f)\n",
    "print(\"Saving the result\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apsap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "002e5154869e9cd98b5ffcb480717f14f2e08d6977aa36f666ba18333329532a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
